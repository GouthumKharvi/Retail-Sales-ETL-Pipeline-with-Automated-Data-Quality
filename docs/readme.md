
# ğŸ“˜ **README.md â€” Retail Sales ETL Pipeline Project**

## ğŸª Retail Sales ETL Pipeline

End-to-end ETL pipeline designed to process Walmart-style retail sales data, transform it into clean analytical datasets, and load it into a MySQL database for reporting, dashboards, and machine-learning workflows.

---

# ğŸ“‚ **Project Structure**

```
retail_sales_etl_pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Original Kaggle CSVs
â”‚   â”œâ”€â”€ staging/             # Optional staging files
â”‚   â”œâ”€â”€ clean/               # Final cleaned outputs
â”‚   â””â”€â”€ archive/             # Backup storage
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py           # Load raw CSV â†’ staging tables
â”‚   â”œâ”€â”€ transform.py         # Clean/merge/feature engineering
â”‚   â”œâ”€â”€ load.py              # Load clean data into MySQL
â”‚   â””â”€â”€ etl_pipeline.py      # Combined Extract â†’ Transform â†’ Load
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_tables.sql    # DDL for staging & clean tables
â”‚   â”œâ”€â”€ staging_insert.sql   # Insert raw data into staging
â”‚   â”œâ”€â”€ transformations.sql  # SQL cleaning logic (optional)
â”‚   â”œâ”€â”€ load_clean.sql       # Load cleaned dataset into final tables
â”‚   â””â”€â”€ validation_queries.sql # Data quality checks
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ etl_log.txt          # Extract & transform logs
â”‚   â”œâ”€â”€ errors.log           # Error tracking
â”‚   â””â”€â”€ etl_pipeline_log.txt # Pipeline-level logs
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ db_config.json       # DB credentials
â”‚   â””â”€â”€ etl_config.json      # Paths & rules
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture_diagram.png   # ETL architecture
â”‚   â”œâ”€â”€ entity_relationship.png    # ER diagram
â”‚   â”œâ”€â”€ README.md                  # Extended documentation
â”‚   â””â”€â”€ interview_summary.md       # How to explain in interview
â”‚
â””â”€â”€ README.md
```

---

# ğŸ¯ **Project Overview**

### âœ” **Objective**

Build a complete ETL pipeline to:

* Collect raw retail sales, store data, and feature data
* Clean, validate, merge, and enrich the dataset
* Load it into a MySQL database
* Prepare it for analytics, dashboards, and machine-learning models

### âœ” **Datasets Used**

From Kaggle Walmart Retail Dataset:

* `train.csv`
* `test.csv`
* `features.csv`
* `stores.csv`

---

# ğŸ”§ **Technologies Used**

| Layer       | Tools                       |
| ----------- | --------------------------- |
| Programming | Python (pandas, SQLAlchemy) |
| Database    | MySQL 8                     |
| Logging     | Python logging + log files  |
| Versioning  | Git / GitHub                |
| Optional    | Jupyter Notebooks for EDA   |

---

# ğŸš€ **Pipeline Workflow**

## **1ï¸âƒ£ Extract (extract.py)**

* Reads raw CSV files
* Normalizes column names
* Loads data into MySQL staging tables:

  * `sales_staging`
  * `features_staging`
  * `stores_staging`

## **2ï¸âƒ£ Transform (transform.py)**

* Converts datatypes
* Handles missing values
* Standardizes date formats
* Merges:

  * sales + features
  * * store dimension
* Outputs cleaned files:

```
data/clean/sales_clean.csv
data/clean/features_clean.csv
data/clean/stores_clean.csv
data/clean/full_dataset_clean.csv
```

## **3ï¸âƒ£ Load (load.py)**

Loads cleaned datasets into MySQL final tables:

* `sales_clean`
* `features_clean`
* `dim_store`
* (optional) `fact_sales`
* (optional) `full_dataset_clean`

## **4ï¸âƒ£ Combined Script (etl_pipeline.py)**

Runs:

```
extract â†’ transform â†’ load
```

in sequence with logging.

---

# ğŸ—„ **Database Schema**

### Clean Tables

* **sales_clean**
* **features_clean**
* **dim_store**

### Relationships

* `sales_clean.store` â†’ `dim_store.store`
* `features_clean.store` â†’ `dim_store.store`
* `sales_clean.sale_date` â†” `features_clean.feature_date`

(See `docs/entity_relationship.png`)

---

# âœ” **How to Run the ETL Pipeline**

### Step 1 â€” Activate environment

```cmd
conda activate base
```

### Step 2 â€” Go to scripts folder

```cmd
cd retail_sales_etl_pipeline/scripts
```

### Step 3 â€” Run pipeline

```cmd
python etl_pipeline.py
```

### Successful Output

```
ETL PIPELINE COMPLETED SUCCESSFULLY
```

---

# âœ“ **Data Validation**

Validation scripts include:

* Row count checks
* NULL value checks
* Date mismatches
* Orphan stores
  (run via SQL or scripts)

---

# ğŸ“Š **Notebooks**

Two Jupyter notebooks perform:

* **exploratory_analysis.ipynb** â†’ raw data EDA
* **validation.ipynb** â†’ final data validation

---

# ğŸ§‘â€ğŸ’¼ **Interview Summary**

A short, clear explanation of your entire project is inside:

```
docs/interview_summary.md
```

---

# â­ **Project Status**

âœ” ETL complete
âœ” SQL complete
âœ” Clean data generated
âœ” Logs working
âœ” ER diagram + architecture diagram
â¬œ Notebooks (optional)
â¬œ Cloud deployment (optional)
â¬œ Dashboard (Power BI / Tableau)

---

# ğŸ **Conclusion**

This project delivers a **full production-grade ETL system** with:

* automated ingestion
* robust transformations
* reliable loading
* clean database design
* reusable scripts
* and interview-ready documentation

