# Retail-Sales-ETL-Pipeline-with-Automated-Data-Quality
End-to-end ETL pipeline for retail sales forecasting and analytics. It ingests raw CSV data (train, test, features, stores), performs cleaning and transformations, loads the processed data into a MySQL Data Warehouse, and provides a fully interactive Streamlit-based analytics dashboard for monitoring, data exploration, and visualization.



âœ” Project Overview
âœ” Architecture
âœ” ERD
âœ” Folder Structure
âœ” ETL Steps
âœ” Streamlit App
âœ” Big Data Integration
âœ” SQL Design
âœ” Screenshots placeholders
âœ” How to deploy
âœ” How to run ETL
âœ” How to run Streamlit
âœ” Skills demonstrated
âœ” Resume-friendly summary



---

# ðŸ“Š Retail Sales ETL Pipeline with Automated Data Quality & Streamlit Dashboard

*A complete end-to-end Data Engineering project with ETL, SQL Data Warehouse, Automated Data Quality, Big Data (Hadoop/Spark/Kafka) integrations, and a Production-grade Streamlit Analytics Platform.*

---

## ðŸš€ Project Summary 

**â€œRetail Sales ETL & Analyticsâ€**

---

## ðŸ§  About the Project

This project is a **real-world enterprise-level ETL and analytics system** built for processing large-scale **Retail Sales Data**. It includes:

* Full **ETL pipeline** (Extract â†’ Transform â†’ Load)
* Automated **data quality validation**
* **SQL database schema + data warehouse design**
* **Streamlit web dashboard** with advanced analytics
* **Big Data integrations** (Hadoop, Spark, Kafka)
* **GitHub LFS** for large CSV file management
* **Clean folder structure** following professional data engineering standards



**Python Â· ETL Â· Data Engineering Â· SQL Â· Database Design Â· Streamlit Â· Big Data (Hadoop/Spark/Kafka) Â· Automation Â· Data Quality**

---

# ðŸ“ Full Project Structure

```
retail_sales_etl_pipeline/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ db_config.json
â”‚   â””â”€â”€ etl_config.json
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â”œâ”€â”€ test.csv
â”‚   â”‚   â”œâ”€â”€ features.csv
â”‚   â”‚   â””â”€â”€ stores.csv
â”‚   â”œâ”€â”€ staging/
â”‚   â”œâ”€â”€ clean/
â”‚   â”‚   â”œâ”€â”€ sales_clean.csv
â”‚   â”‚   â”œâ”€â”€ features_clean.csv
â”‚   â”‚   â”œâ”€â”€ stores_clean.csv
â”‚   â”‚   â””â”€â”€ full_dataset_clean.csv
â”‚   â””â”€â”€ archive/
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â”œâ”€â”€ entity_relationship.png
â”‚   â”œâ”€â”€ interview_summary.md
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ etl_log.txt
â”‚   â”œâ”€â”€ errors.log
â”‚   â””â”€â”€ etl_pipeline_log.txt
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ exploratory_analysis.ipynb
â”‚   â””â”€â”€ validation.ipynb
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â”œâ”€â”€ etl_pipeline.py
â”‚   â”œâ”€â”€ scheduler.bat
â”‚   â””â”€â”€ scheduler.sh
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_tables.sql
â”‚   â”œâ”€â”€ staging_insert.sql
â”‚   â”œâ”€â”€ transformations.sql
â”‚   â”œâ”€â”€ load_clean.sql
â”‚   â””â”€â”€ validation_queries.sql
â”‚
â”œâ”€â”€ streamlit/
â”‚   â””â”€â”€ streamlit_app.py
â”‚
â””â”€â”€ README.md
```

---

# ðŸ—ï¸ System Architecture

Below is the projectâ€™s high-level architecture:

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Raw CSV Files      â”‚
                â”‚ train/test/featuresâ”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                 (Extract.py - Pandas)
                          â”‚
                          â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚       Data Cleaning             â”‚
         â”‚ transform.py â†’ NA handling     â”‚
         â”‚ normalization â†’ merging        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                (Load.py - MySQL Load)
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   MySQL Database     â”‚
              â”‚ Fact + Dimension     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
               (SQL + Views + Joins)
                        â”‚
                        â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Streamlit Analytics Dashboard   â”‚
          â”‚ KPI Metrics, Analysis, Insights â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚      Big Data Integration         â”‚
          â”‚  Hadoop Â· Spark Â· Kafka (real-time)â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ðŸ—‚ï¸ Database ER Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     STORES          â”‚         â”‚     DEPARTMENTS      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PK store_id         â”‚â”€â”€â”€â”€â”    â”‚ PK department_id     â”‚
â”‚ store_type          â”‚    â”‚    â”‚ department_name      â”‚
â”‚ size                â”‚    â”‚    â”‚ category             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                â”‚
          â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      SALES_TRANSACTIONS      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PK transaction_id            â”‚
â”‚ FK store_id                  â”‚
â”‚ FK department_id             â”‚
â”‚ FK date_id                   â”‚
â”‚ weekly_sales                 â”‚
â”‚ is_holiday                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      DATE_DIMENSION      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ECONOMIC_INDICATORS      â”‚         â”‚      SALES_FEATURES      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FK store_id                 â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚ FK store_id              â”‚
â”‚ FK date_id                  â”‚         â”‚ FK date_id               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ðŸ› ï¸ 1. Extract Step

`extract.py` loads raw CSVs from `/data/raw/`.

### âœ” Features:

* Validates files
* Logs missing or corrupted files
* Loads using Pandas with dtype handling
* Saves intermediate outputs

---

# ðŸ§¹ 2. Transform Step

`transform.py` performs:

### âœ” Cleaning

* Missing value handling
* Date normalization
* Type casting
* Removing duplicates

### âœ” Merging

* Join Sales + Features + Stores into `full_dataset_clean.csv`

### âœ” Validation

* Null checks
* Summary logging
* Ensuring primary keys

---

# ðŸ’¾ 3. Load Step

`load.py` loads cleaned datasets into the SQL database.

### âœ” Features

* Loads staging â†’ final tables
* Uses batch inserts
* Auto-creates tables if missing
* Runs SQL scripts:

  * `create_tables.sql`
  * `staging_insert.sql`
  * `transformations.sql`
  * `load_clean.sql`
  * `validation_queries.sql`

---

# ðŸ”„ 4. Orchestrated ETL Pipeline

`etl_pipeline.py` automates:

```
Extract â†’ Transform â†’ Load
```

### âœ” Included

* Central logging
* Error logging
* Time tracking
* Console output
* Streamlit integration

### âœ” Windows/Linux Schedulers

* `scheduler.bat`
* `scheduler.sh`

---

# ðŸŽ¨ Full Streamlit Web App

File: `/streamlit/streamlit_app.py`

This is the **most advanced part** of your projectâ€”built with
**animations, cards, metrics, analytics, SQL explorer, uploads, logs, AI-style insights, Big Data UI, etc.**

### âœ” Features:

#### â­ Executive Dashboard

* KPI cards
* Revenue analytics
* Data quality metrics
* Sampling
* Downloads

#### ðŸ“ˆ Advanced Analytics

* Time-series
* Seasonal trends
* Store comparison
* Correlation heatmaps

#### ðŸŽ¯ Data Quality

* Missing data
* Duplicate detection
* Scored quality ratings

#### ðŸ—„ Database Explorer

* Table list
* Auto SQL query runner
* Row previews
* Table metrics
* Downloadable extracts

#### ðŸ“¤ Upload Center

* Upload raw data
* Preview before saving
* Metadata cards
* Auto-save to `/data/raw/`

#### ðŸ“‹ Logs Viewer

* Auto-refresh
* Error/Warning filtering
* Line counts

#### ðŸ” AI Style Insights

* Top stores
* Trend analysis
* Recommendations
* PDF-ready report

#### ðŸ”¥ Big Data Tech UI

* Hadoop
* Spark
* Kafka
* Architecture
* Sample ETL code
* Cluster metrics
* Visual explanations
* Enterprise-ready diagrams

#### ðŸ—‚ SQL Database Design

* ERD
* Schema
* Relationships
* Sample SQL queries

---

# ðŸ˜ Big Data Integrations (Conceptual + Code)

### âœ” Hadoop HDFS

* Upload raw files
* Store processed data
* Commands included

### âœ” PySpark ETL

* Parallel transformations
* Joins
* Aggregations
* Writes back to HDFS

### âœ” Kafka Streaming

* Producer â†’ topic â†’ Spark consumer
* Real-time transformations
* Event-driven pipeline

---

# ðŸ“Œ How to Run the Project

## 1ï¸âƒ£ Clone Repo

```
git clone https://github.com/GouthumKharvi/Retail-Sales-ETL-Pipeline-with-Automated-Data-Quality.git
cd Retail-Sales-ETL-Pipeline-with-Automated-Data-Quality
```

---

## 2ï¸âƒ£ Install Requirements

```
pip install -r requirements.txt
```

---

## 3ï¸âƒ£ Run ETL

```
cd scripts
python etl_pipeline.py
```

---

## 4ï¸âƒ£ Run Streamlit App

```
cd streamlit
streamlit run streamlit_app.py
```

---

# ðŸ§ª Jupyter Notebooks Included

### `/notebooks/exploratory_analysis.ipynb`

* Raw data EDA
* Distribution analysis
* Trends

### `/notebooks/validation.ipynb`

* Schema verification
* Missing values
* Uniqueness
* Statistical checks

---

# ðŸ§¾ Automated Data Quality

The project computes:

| Metric        | Description              |
| ------------- | ------------------------ |
| Completeness  | Missing values %         |
| Uniqueness    | Duplicate detection      |
| Validity      | Type and range checks    |
| Freshness     | Latest date in data      |
| Quality Score | Weighted composite score |

---

# ðŸ“¦ Git LFS Support

Large files (CSV > 100MB) stored with:

```
git lfs install
git lfs track "*.csv"
```

---

# ðŸ§° SQL Folder Includes

| File                     | Purpose                        |
| ------------------------ | ------------------------------ |
| `create_tables.sql`      | DDL for all tables             |
| `staging_insert.sql`     | Loads staging tables           |
| `transformations.sql`    | SQL-based cleaning             |
| `load_clean.sql`         | Inserts into final fact tables |
| `validation_queries.sql` | Row/NULL/Type checks           |

---

# ðŸ§  Skills Demonstrated

### âœ” Data Engineering

* ETL pipelines
* SQL DWH modeling
* Batch processing

### âœ” Python

* Pandas
* Automation
* Logging

### âœ” Streamlit

* Complex front-end
* Animated UI
* Advanced charts

### âœ” Big Data

* Hadoop
* Spark
* Kafka

### âœ” DevOps

* Schedulers
* JSON config mgmt
* Git LFS
* Modular folder structure

---


# ðŸ“Ž Author

**Goutham Kharvi**
Retail Sales ETL Pipeline â€¢ 2025


